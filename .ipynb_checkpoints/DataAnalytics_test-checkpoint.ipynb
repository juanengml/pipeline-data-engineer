{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9eec3119",
   "metadata": {},
   "source": [
    "# ST IT Cloud - Data and Analytics Test LV.4\n",
    "\n",
    "Esse teste deve avaliar alguns conceitos de big data e a qualidade técnica na manipulacão de dados, otimização de performance, trabalho com arquivos grandes e tratamento de qualidade.\n",
    "\n",
    "## Passo a passo\n",
    "\n",
    "- *Parte teórica:* responda as questões abaixo preenchendo as células em branco.\n",
    "- *Parte prática:* disponibilizamos aqui 2 cases para, leia os enunciados dos problemas, desenvolver os programas, utilizando a **stack definida durante o processo seletivo**, para entregar os dados de acordo com os requisitos descritos abaixo.\n",
    "\n",
    "\n",
    "\n",
    "**Faz parte dos critérios de avaliacão a pontualidade da entrega. Implemente até onde for possível dentro do prazo acordado.**\n",
    "\n",
    "**Os dados de pessoas foram gerados de forma aleatória, utilizando a biblioteca FakerJS, FakerJS-BR e Faker**\n",
    "\n",
    "LEMBRE-SE: A entrega deve conter TODOS os passos para o avaliador executar o programa (keep it simple).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9447dec4",
   "metadata": {},
   "source": [
    "**Questão 1** - Descreva de forma detalhada quais são as etapas na construção de um pipeline de dados, sem considerar ferramentas específicas, imagine que é seu primeiro contato com o cliente e você precisa entender a demanda dele e explicar quais são os passos que você terá que implementar para entregar a demanda."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c986e8e0-94cb-4f86-aa94-e02cd83cd04a",
   "metadata": {},
   "source": [
    "Quando penso em um data pipeline, penso em 4 etapas. \n",
    "\n",
    "- Data Source: Nessa etapa é importante entender os dados, de onde vem, qual a fonte de dados, problematica e suas respectivas definições.\n",
    "\n",
    "- Data Cleanning e Data Quality: Aqui é feito o processo de limpeza de dados, organização, remoção de duplicatas. Aqui tambem podemos fazer o processo de enriquecimento dos dados e respectivas transformações.\n",
    "\n",
    "- Data Delivery: Por fim essa ultima etapa é a forma de como os dados já estaveis e equalizados podem ser servidos, tanto em BI, feature Store e mesmo outras formas de apresentação dos dados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc703af3",
   "metadata": {},
   "source": [
    "**Questão 2** - Defina com suas palavras um processamento em streaming e processamento em batch. Qual sua experiência com cada uma delas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fd95fb-07c3-4ce0-871e-a7dbbd4f8551",
   "metadata": {},
   "source": [
    "- Processamento em Streaming: Tudo processamento que precisa ser feito em realtime, tudo o que precisa ser feito em segundos ou em microsegundos, aqui entra as estruturas em filas,apis, mensagerias, listas e outras estruturas que precisam ser processadas em tempo real. \n",
    "  \n",
    "  Sobre Streaming: Tenho experiencia em construção de apis para processos de ingestão e inferencia, construção de workers pluganos a filas como RabbitMQ/Kafka/EventHub.  \n",
    "  \n",
    "- Processamento em Batch: Tudo que não precisa rodar em tempo real, pode rodar acada 1h, 1d, ou 1 vez ao mes, aqui o processo e ter uma massa de dados que precisa ser processada em intervalos de tempo. EX: Inferencia de dados de fraude acada 1h no varejo para saber se um compra é fraudulenta ou não. EX: Retreino de modelos com intervalo em estações do ano.\n",
    "\n",
    "   Sobre Batch: Tenho experiencia em montar pipelines em batch, com schedule manual e automatico, para operações de MLOps para retreino. Gerenciar estrutura de retreino para modelos de detecção de objetos, (labeling, retreino, deploy)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1c3f10",
   "metadata": {},
   "source": [
    "**Questão 3** - Quais são as camadas de um Data Lake?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a47ecf9-3f59-46b5-ab1f-513c3bc98807",
   "metadata": {},
   "source": [
    "Camadas de um Data lake pode ser definidas em\n",
    "\n",
    "Data Source, Data Cleanning, Data Quality e Data Delivery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62477089",
   "metadata": {},
   "source": [
    "**Questão 4** - Quais as diferenças de um Data Lake e um DW?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d401324-cc4d-4361-8bfc-87f2666feca4",
   "metadata": {},
   "source": [
    "Datalake: Recebe todo tipo de dado, sejam dados não estruturados, estruturados, como tabelas, planilhas, arquivos de video, imagem, qualquer que seja a forma de dado.\n",
    "\n",
    "Data warehouse: Apenas dados estruturados, tabular, ou com algum tipo de estrutura"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7099a05e",
   "metadata": {},
   "source": [
    "**Questão 5** - O que é arquitetura Lambda e Kappa? Descreva com suas palavras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8489495-9a19-465c-8d82-de29baa46d47",
   "metadata": {},
   "source": [
    "Arquitetura Lambda: \n",
    "\n",
    "    Camada de Batch: Recebe os dados e armazena na sua estrutura e envia os dados para camada de serving\n",
    "    \n",
    "    Camada de Serving: Recebe os dados e suas respectivas querys podem ser feitos pelo usuario permitindo sua vizualização. E por final é enviado para camada de Speed Layer\n",
    "    \n",
    "    Camada Rapida: Aqui os dados são dedicados para consumo em realtime \n",
    "    \n",
    "Arquitetura Kappa: \n",
    "    \n",
    "    Camada de Streaming de dados: recebe os dados e panda para camada rapida \n",
    "    \n",
    "    Camada de Serving: Recebe as queryes tambem e registra os dados na camada de armazenamento \n",
    "    \n",
    "    Camada de armazenamento: Aqui os dados são salvos e direcionados para suas respectivas tabelas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3d483e",
   "metadata": {},
   "source": [
    "**Questão 6** - O que é Data Quality para você e como você implementa isso nos seus processos?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03be9b26-7bfd-4638-af0f-62e6cd7de202",
   "metadata": {},
   "source": [
    "Data Quality é o processo de qualidade dos dados, onde é checados dados nulos, repetidos, valores trocados. Esse processo é bem extenço é muitas vezes bem trabalhoso. Um exemplo que sempre uso em processos antes de retreinar qualquer modelos de visão computacional, \n",
    "\n",
    "* Checar Qualidade da Image\n",
    "* Checar Ruido na imagem\n",
    "* Checar oclusão da imagem\n",
    "* Remover Ruido\n",
    "* Remover falsos positivos\n",
    "* Remover imagens com baixa qualidade\n",
    "* Remover frames sem nenhuma informação(com oclusão, borrados)\n",
    "* Splitar frames em contagens distintas(EX: a cada 20 frames salve 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4834d6",
   "metadata": {},
   "source": [
    "**Questão 7** - Em uma escala de 0 a 10, qual seria seu nível de experiência com PySpark?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c02e1b-86b7-4d91-bf62-39dc41e0c41b",
   "metadata": {},
   "source": [
    "R: 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ef78ba",
   "metadata": {},
   "source": [
    "**Questão 8** - Em uma escala de 0 a 10, qual seria seu nível de experiência com SQL?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a407b0d-22f2-434e-9a09-b5e2aba762b0",
   "metadata": {},
   "source": [
    "R: 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5f7ee6",
   "metadata": {},
   "source": [
    "**Questão 9** - Descreva suas expeciências com banco de dados SQL e NoSQL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4e36d5-bd4f-4899-ba3f-cd579664ab7e",
   "metadata": {},
   "source": [
    "Tenho mais experiencias com Bancos Relacionais, com MySQL e SQLSERVER, na globo usavamos muito SQLServer, na via usamos SQL Spark para fazer consultas no DeltaLake\n",
    "\n",
    "Tambem sei um pouco de NoSQL, principalmente queries em ElasticSearch com Lucene, na via usamos para telemetria e logs de modelos e comportamento de pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2fa6e5",
   "metadata": {},
   "source": [
    "**Questão 10** - Tem experiência com versionamento de código? Com quais ferramentas já trabalhou? Descreva."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770b4c2a-220b-40ea-b090-f9e74876cfb1",
   "metadata": {},
   "source": [
    "- Sim, conheço Github, Gitlab, Bitbucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe2714f",
   "metadata": {},
   "source": [
    "**Questão 11** - Tem experiência em desenvolvimento em cloud? Se sim, especifique a(s) plataforma(s) que já trabalhou e suas principais implementações e conhecimentos em cada serviço."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd3b066-cd15-4d11-bc45-92c8fa5e0b8f",
   "metadata": {},
   "source": [
    "* AWS: Conheço EC2, S3, Lambda, Api gateway, ecs, ses,sqs. Dessas ferramentas já montei uma automação usando um bucket s3 que recebe videos novos a cada 1 semana e a cada 1 vez por mês o retreino é feito usando lambda para subir as maquinas e usando sqs para publicar quais dados novos precisam passar para retreino de modelo, com modelo novo, é versionado no mlflow e consumido para uma api.\n",
    "\n",
    "* Azure: Conheço Databricks, data factory, ACR e AKS. Aqui conheço mais data factory, usamos muito na via, todo processo de etl construido pelo time DS e montado em cima do data factory. Inferencia tambem. Basicamente é quase a mesma estrutura deste teste, com pipelines e seus respectivos notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9bc70d",
   "metadata": {},
   "source": [
    "**Questão 12** - Tem experiência com metodologia ágil? Qual?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122a0779-04a5-4b45-ad5f-c74247adfbac",
   "metadata": {},
   "source": [
    "* Conheço, SAFE, OKR, Scrum e Kanban"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb61f06",
   "metadata": {},
   "source": [
    "# TESTE PRÁTICO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed8c6a5",
   "metadata": {},
   "source": [
    "**Problema 1**: Você está recebendo o arquivo 'dados_cadastrais_fake.csv' que contem dados cadastrais de clientes, mas para que análises ou relatórios sejam feitos é necessário limpar e normalizar os dados. Além disso, existe uma coluna com o número de cpf e outra com cnpj, você precisará padronizar deixando apenas dígitos em formato string (sem caracteres especiais), implementar uma forma de verificar se tais documentos são válidos sendo que a informação deve se adicionada ao dataframe em outras duas novas colunas.\n",
    "\n",
    "Após a normalização, gere reports que respondam as seguintes perguntas:\n",
    "- Quantos clientes temos nessa base?\n",
    "- Qual a média de idade dos clientes?\n",
    "- Quantos clientes nessa base pertencem a cada estado?\n",
    "- Quantos CPFs válidos e inválidos foram encontrados?\n",
    "- Quantos CNPJs válidos e inválidos foram encontrados?\n",
    "\n",
    "Ao final gere um arquivo no formato csv e um outro arquivo no formato parquet chamado (problema1_normalizado), eles serão destinados para pessoas distintas.\n",
    "\n",
    "*EXTRA:* executar as mesmas validações no *1E8.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1b6c2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2da9c40",
   "metadata": {},
   "source": [
    "**Problema 2**: Você deverá implementar um programa, para ler, tratar e particionar os dados.\n",
    "\n",
    "O arquivo fonte está disponível em `https://st-it-cloud-public.s3.amazonaws.com/people-v2_1E6.csv.gz`\n",
    "\n",
    "### Data Quality\n",
    "\n",
    "- Higienizar e homogenizar o formato da coluna `document`\n",
    "- Detectar através da coluna `document` se o registro é de uma Pessoa Física ou Pessoa Jurídica, adicionando uma coluna com essa informação\n",
    "- Higienizar e homogenizar o formato da coluna `birthDate`\n",
    "- Existem duas colunas nesse dataset que em alguns registros estão trocadas. Quais são essas colunas? \n",
    "- Corrigir os dados com as colunas trocadas\n",
    "- Além desses pontos, existem outras tratamentos para homogenizar esse dataset. Aplique todos que conseguir.\n",
    "\n",
    "### Agregação dos dados\n",
    "\n",
    "- Quais são as 5 PF que mais gastaram (`totalSpent`)? \n",
    "- Qual é o valor de gasto médio por estado (`state`)?\n",
    "- Qual é o valor de gasto médio por `jobArea`?\n",
    "- Qual é a PF que gastou menos (`totalSpent`)?\n",
    "- Quantos nomes e documentos repetidos existem nesse dataset?\n",
    "- Quantas linhas existem nesse dataset?\n",
    "\n",
    "### Particionamento de dados tratados com as regras descritas em `DATA QUALITY`\n",
    "\n",
    "- Particionar em arquivos PARQUET por estado (`state`)\n",
    "- Particionar em arquivos CSV por ano/mes/dia de nascimento (`birthDate`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2277f816",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
